{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, TextVectorization, Dropout, Dense, Layer\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('She kicked him hard.', '[start] Ella le pateó con saña. [end]')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Can you sleep?', '[start] ¿Puedes dormir? [end]'),\n",
       " ('They replaced coal with oil.',\n",
       "  '[start] Reemplazaron al carbón por el aceite. [end]')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "train_pairs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(input_string):\n",
    "    \n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    \n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the English and Spanish text pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "target_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, \n",
    "                                         standardize=standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you sleep?\n",
      "[start] ¿Puedes dormir? [end]\n"
     ]
    }
   ],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "print(train_english_texts[0])\n",
    "print(train_spanish_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    \n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    \n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(pairs):\n",
    "    \n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = Sequential([Dense(dense_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        \n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        \n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super().get_config()\n",
    "        \n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim\n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(Layer):\n",
    "    \n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = Sequential([Dense(dense_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        \n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        \n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        \n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        \n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "        \n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, \n",
    "                                              key=encoder_outputs, attention_mask=padding_mask)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        \n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**PositionalEmbedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(Layer):\n",
    "    \n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        \n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        \n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**End-to-end Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "Transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the sequence-to-sequence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 57s 39ms/step - loss: 1.6845 - accuracy: 0.4151 - val_loss: 1.3251 - val_accuracy: 0.5044\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 1.3400 - accuracy: 0.5325 - val_loss: 1.1636 - val_accuracy: 0.5682\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 1.1856 - accuracy: 0.5828 - val_loss: 1.0784 - val_accuracy: 0.6026\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 1.0943 - accuracy: 0.6161 - val_loss: 1.0398 - val_accuracy: 0.6242\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 1.0451 - accuracy: 0.6377 - val_loss: 1.0137 - val_accuracy: 0.6374\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 1.0144 - accuracy: 0.6539 - val_loss: 1.0083 - val_accuracy: 0.6405\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 51s 39ms/step - loss: 0.9917 - accuracy: 0.6665 - val_loss: 0.9985 - val_accuracy: 0.6482\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 50s 39ms/step - loss: 0.9735 - accuracy: 0.6767 - val_loss: 0.9996 - val_accuracy: 0.6499\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.9579 - accuracy: 0.6859 - val_loss: 0.9965 - val_accuracy: 0.6535\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 50s 38ms/step - loss: 0.9445 - accuracy: 0.6924 - val_loss: 0.9946 - val_accuracy: 0.6564\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.9304 - accuracy: 0.7001 - val_loss: 1.0094 - val_accuracy: 0.6554\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.9182 - accuracy: 0.7052 - val_loss: 0.9980 - val_accuracy: 0.6621\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.9063 - accuracy: 0.7109 - val_loss: 1.0013 - val_accuracy: 0.6604\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 50s 38ms/step - loss: 0.8948 - accuracy: 0.7163 - val_loss: 1.0069 - val_accuracy: 0.6604\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8838 - accuracy: 0.7206 - val_loss: 1.0095 - val_accuracy: 0.6636\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8735 - accuracy: 0.7251 - val_loss: 1.0226 - val_accuracy: 0.6591\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8628 - accuracy: 0.7297 - val_loss: 1.0204 - val_accuracy: 0.6622\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8532 - accuracy: 0.7334 - val_loss: 1.0212 - val_accuracy: 0.6605\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 50s 38ms/step - loss: 0.8439 - accuracy: 0.7369 - val_loss: 1.0337 - val_accuracy: 0.6633\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8348 - accuracy: 0.7404 - val_loss: 1.0177 - val_accuracy: 0.6649\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 50s 38ms/step - loss: 0.8257 - accuracy: 0.7436 - val_loss: 1.0353 - val_accuracy: 0.6624\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8172 - accuracy: 0.7471 - val_loss: 1.0306 - val_accuracy: 0.6667\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8093 - accuracy: 0.7500 - val_loss: 1.0299 - val_accuracy: 0.6621\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.8008 - accuracy: 0.7530 - val_loss: 1.0405 - val_accuracy: 0.6642\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7932 - accuracy: 0.7560 - val_loss: 1.0398 - val_accuracy: 0.6638\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7862 - accuracy: 0.7587 - val_loss: 1.0522 - val_accuracy: 0.6659\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7786 - accuracy: 0.7610 - val_loss: 1.0491 - val_accuracy: 0.6669\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7713 - accuracy: 0.7637 - val_loss: 1.0520 - val_accuracy: 0.6634\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7643 - accuracy: 0.7665 - val_loss: 1.0581 - val_accuracy: 0.6637\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 49s 38ms/step - loss: 0.7582 - accuracy: 0.7687 - val_loss: 1.0577 - val_accuracy: 0.6642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f97aef490>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "Transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Translating new sentences with our Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        \n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = Transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        \n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "No one was listening.\n",
      "[start] nadie estaba escuchando [end]\n",
      "-\n",
      "I'm living in Boston.\n",
      "[start] estoy estudiando en boston [end]\n",
      "-\n",
      "He began to work for that company last year.\n",
      "[start] Él empezó a trabajar a esa compañía el año pasado [end]\n",
      "-\n",
      "I'm sure that he went to Tokyo.\n",
      "[start] estoy seguro de que fue a tokio [end]\n",
      "-\n",
      "Don't spend more than you earn.\n",
      "[start] no te es más que [UNK] [end]\n",
      "-\n",
      "Read this.\n",
      "[start] [UNK] esto [end]\n",
      "-\n",
      "He is looking for a job.\n",
      "[start] Él está buscando trabajo [end]\n",
      "-\n",
      "They found each other.\n",
      "[start] ellos se saben [end]\n",
      "-\n",
      "Please don't ask me.\n",
      "[start] por favor no me [UNK] [end]\n",
      "-\n",
      "My friend is seventeen years old.\n",
      "[start] mi amigo tiene estados unidos [end]\n",
      "-\n",
      "Even a child can understand that.\n",
      "[start] incluso un niño puede entender eso [end]\n",
      "-\n",
      "She's the most popular girl in the class.\n",
      "[start] es la chica más popular en las clases [end]\n",
      "-\n",
      "Tom was in town Monday night.\n",
      "[start] tom estaba en la ciudad el lunes por la noche [end]\n",
      "-\n",
      "I know something you don't know.\n",
      "[start] sé algo que no saben [end]\n",
      "-\n",
      "He is outgoing.\n",
      "[start] Él está [UNK] [end]\n",
      "-\n",
      "Aren't you tired?\n",
      "[start] no estás cansado [end]\n",
      "-\n",
      "What a clever dog!\n",
      "[start] ¡qué padre tan bueno [end]\n",
      "-\n",
      "Somebody messed up.\n",
      "[start] alguien [UNK] [end]\n",
      "-\n",
      "\"What time is it?\" \"It's 3:20.\"\n",
      "[start] qué hora es es un [UNK] [end]\n",
      "-\n",
      "You need to open an account at a bank to receive the payment.\n",
      "[start] debes venir a tom en un banco [UNK] a la parte de los [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part04_sequence-to-sequence-learning.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
