{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### A machine translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, TextVectorization, Bidirectional, GRU, Dropout, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Can you recommend a good play?', '[start] ¿Puedes recomendar una buena obra de teatro? [end]')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I have two brothers and one sister.',\n",
       "  '[start] Tengo dos hermanos y una hermana. [end]'),\n",
       " ('When do you usually get off work?',\n",
       "  '[start] ¿A qué hora sale comúnmente del trabajo? [end]')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "train_pairs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(input_string):\n",
    "    \n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    \n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the English and Spanish text pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "target_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, \n",
    "                                         standardize=standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have two brothers and one sister.\n",
      "[start] Tengo dos hermanos y una hermana. [end]\n"
     ]
    }
   ],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "print(train_english_texts[0])\n",
    "print(train_spanish_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing datasets for the translation task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    \n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    \n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(pairs):\n",
    "    \n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**GRU-based encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = Bidirectional(GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**GRU-based decoder and the end-to-end model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "past_target = Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "Decoder = GRU(latent_dim, return_sequences=True)\n",
    "x = Decoder(x, initial_state=encoded_source)\n",
    "x = Dropout(0.5)(x)\n",
    "target_next_step = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training our recurrent sequence-to-sequence model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 76s 49ms/step - loss: 1.6332 - accuracy: 0.4181 - val_loss: 1.3188 - val_accuracy: 0.5033\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 1.3123 - accuracy: 0.5288 - val_loss: 1.1545 - val_accuracy: 0.5680\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 1.1715 - accuracy: 0.5774 - val_loss: 1.0758 - val_accuracy: 0.5991\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 1.0824 - accuracy: 0.6087 - val_loss: 1.0422 - val_accuracy: 0.6166\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 1.0350 - accuracy: 0.6328 - val_loss: 1.0248 - val_accuracy: 0.6283\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 1.0037 - accuracy: 0.6518 - val_loss: 1.0228 - val_accuracy: 0.6324\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9842 - accuracy: 0.6652 - val_loss: 1.0202 - val_accuracy: 0.6370\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 61s 47ms/step - loss: 0.9710 - accuracy: 0.6753 - val_loss: 1.0227 - val_accuracy: 0.6385\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9610 - accuracy: 0.6828 - val_loss: 1.0275 - val_accuracy: 0.6403\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9529 - accuracy: 0.6892 - val_loss: 1.0272 - val_accuracy: 0.6427\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9475 - accuracy: 0.6937 - val_loss: 1.0305 - val_accuracy: 0.6427\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9433 - accuracy: 0.6966 - val_loss: 1.0334 - val_accuracy: 0.6429\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 58s 45ms/step - loss: 0.9409 - accuracy: 0.6985 - val_loss: 1.0371 - val_accuracy: 0.6438\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 60s 46ms/step - loss: 0.9395 - accuracy: 0.7002 - val_loss: 1.0383 - val_accuracy: 0.6434\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 59s 45ms/step - loss: 0.9390 - accuracy: 0.7012 - val_loss: 1.0398 - val_accuracy: 0.6432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18535cc3a30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Translating new sentences with our RNN encoder and decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "    \n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        \n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = model.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "    \n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "How many hours did you spend with Tom?\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[start] cuántas horas [UNK] con tom [end]\n",
      "-\n",
      "What's your favorite musical instrument?\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[start] cuál es tu [UNK] favorito [end]\n",
      "-\n",
      "Is she healthy?\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[start] ella está es [end]\n",
      "-\n",
      "Let's not pretend otherwise.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[start] no [UNK] lo que [UNK] [end]\n",
      "-\n",
      "This is the most interesting story that I have ever read.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[start] esta es la historia más interesante que he visto de leer [end]\n",
      "-\n",
      "I've heard of you.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[start] he oído de ti [end]\n",
      "-\n",
      "I know it wasn't easy.\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[start] sé que no fue fácil [end]\n",
      "-\n",
      "We went to the park to play.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[start] fuimos al parque a jugar [end]\n",
      "-\n",
      "Can you see what's wrong with this picture?\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[start] puedes ver qué está algo esta [UNK] [end]\n",
      "-\n",
      "I need to buy stamps.\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[start] necesito comprar una de tengo [end]\n",
      "-\n",
      "Tom's life was changed forever.\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[start] la vida de tom se ha ido por ellos [end]\n",
      "-\n",
      "Our team is winning.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[start] nuestro equipo está en qué le va a dar una nosotros [end]\n",
      "-\n",
      "I usually get up at 6:00.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[start] yo lo en cómo se me [UNK] a los [UNK] [end]\n",
      "-\n",
      "Tom changed his mind at the last minute.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[start] tom se puso a mí por el momento [end]\n",
      "-\n",
      "You can take today off.\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[start] puedes [UNK] hoy [end]\n",
      "-\n",
      "I haven't heard of him since then.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[start] no he oído de él desde aquí [end]\n",
      "-\n",
      "I really ought to go.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[start] realmente debería ir [end]\n",
      "-\n",
      "Why do you hate me so much?\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[start] por qué me [UNK] tanto [end]\n",
      "-\n",
      "You should go to school.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[start] deberías ir a la escuela [end]\n",
      "-\n",
      "Don't make any noise.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[start] no hagas ruido [end]\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part04_sequence-to-sequence-learning.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
