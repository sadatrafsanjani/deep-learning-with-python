{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Unigram Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "    \n",
    "        os.makedirs(val_dir / category)\n",
    "        files = os.listdir(train_dir / category)\n",
    "        random.Random(1337).shuffle(files)\n",
    "        num_val_samples = int(0.2 * len(files))\n",
    "        val_files = files[-num_val_samples:]\n",
    "\n",
    "        for fname in val_files:\n",
    "            shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train = text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "validation = text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test = text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the shapes and dtypes of the first batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"In Panic In The Streets Richard Widmark plays U.S. Navy doctor who has his week rudely interrupted with a corpse that contains plague. As cop Paul Douglas properly points out the guy died from two bullets in the chest. That's not the issue here, the two of them become unwilling partners in an effort to find the killers and anyone else exposed to the disease.<br /><br />As was pointed out by any number of people, for some reason director Elia Kazan did not bother to cast the small parts with anyone that sounds like they're from Louisiana. Having been to New Orleans where the story takes place I can personally attest to that. Richard Widmark and his wife Barbara Bel Geddes can be excused because as a Navy doctor he could be assigned there, but for those that are natives it doesn't work.<br /><br />But with plague out there and the news being kept a secret, the New Orleans PD starts a dragnet of the city's underworld. The dead guy came off a ship from Europe and he had underworld connections. A New Orleans wise guy played by Jack Palance jumps to a whole bunch of erroneous conclusions and starts harassing a cousin of the dead guy who is starting to show plague symptoms. Palance got rave reviews in the first film where he received notice.<br /><br />Personally my favorite in this film is Zero Mostel. This happened right before Mostel was blacklisted and around that time he made a specialty of playing would be tough guys who are really toadies. He plays the same kind of role in the Humphrey Bogart film, The Enforcer. Sadly I can kind of identify with Mostel in that last chase scene where he and Palance are being chased down by Widmark, Douglas, and half the New Orleans Police. Seeing the weight challenged Zero trying to keep up with Palance was something else because I'm kind of in Zero's league now in the heft department.<br /><br />Kazan kept the action going at a good clip, there's very little down time in this film. If there was any less it would be an Indiana Jones film. Panic In The Streets won an Oscar for Best Original Screenplay that year.<br /><br />Kazan also made good use of the New Orleans waterfront and the French Quarter. Some of the same kinds of shots are later used in On the Waterfront. In fact Panic In The Streets is about people not squealing when they really should in their own best interest. Very similar again to On the Waterfront.<br /><br />Panic In The Streets does everyone proud who was associated with it. Now why couldn't Elia Kazan get some decent New Orleans sounding people in the small roles.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(max_tokens=20000, output_mode=\"multi_hot\")\n",
    "text_only_train = train.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train = train.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_val = validation.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_test = test.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Inspecting the output of our binary unigram dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Our model-building utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    \n",
    "    inputs = Input(shape=(max_tokens,))\n",
    "    x = Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary unigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2188/2188 [==============================] - 12s 5ms/step - loss: -485.6478 - accuracy: 0.1428 - val_loss: 1212.6777 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -3068.9805 - accuracy: 0.1429 - val_loss: 4562.7329 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -8102.0518 - accuracy: 0.1429 - val_loss: 10049.1387 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -15509.7891 - accuracy: 0.1429 - val_loss: 17661.9336 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -25318.9707 - accuracy: 0.1429 - val_loss: 27405.4238 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -37453.5117 - accuracy: 0.1429 - val_loss: 39255.4062 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -52150.0586 - accuracy: 0.1429 - val_loss: 53279.9922 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2188/2188 [==============================] - 8s 3ms/step - loss: -69292.7266 - accuracy: 0.1429 - val_loss: 69426.7422 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2188/2188 [==============================] - 8s 4ms/step - loss: -89091.8828 - accuracy: 0.1429 - val_loss: 87717.3594 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2188/2188 [==============================] - 7s 3ms/step - loss: -110766.2969 - accuracy: 0.1429 - val_loss: 108108.7969 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eff898a3a0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)]\n",
    "model.fit(binary_1gram_train.cache(), validation_data=binary_1gram_val.cache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 1205.5143 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part01_introduction.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
