{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Conv2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_base_dir = Path(\"cats_vs_dogs_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(new_base_dir / \"train\", image_size=(180, 180), batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(new_base_dir / \"validation\", image_size=(180, 180), batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(new_base_dir / \"test\", image_size=(180, 180), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Printing the list of trainable weights before and after freezing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable weights before freezing the VGG16: 26\n"
     ]
    }
   ],
   "source": [
    "vgg16.trainable = True\n",
    "print(\"Number of trainable weights before freezing the VGG16:\", len(vgg16.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable weights after freezing the VGG16: 0\n"
     ]
    }
   ],
   "source": [
    "vgg16.trainable = False\n",
    "print(\"Number of trainable weights after freezing the VGG16:\", len(vgg16.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "augmentation = Sequential([RandomFlip(\"horizontal\"), RandomRotation(0.1),RandomZoom(0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(180, 180, 3))\n",
    "\n",
    "x = augmentation(inputs)\n",
    "\n",
    "x = preprocess_input(x)\n",
    "\n",
    "x = vgg16(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "63/63 [==============================] - 16s 220ms/step - loss: 18.6574 - accuracy: 0.9040 - val_loss: 15.1693 - val_accuracy: 0.9220\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 14s 225ms/step - loss: 7.5288 - accuracy: 0.9445 - val_loss: 9.9590 - val_accuracy: 0.9470\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 4.4232 - accuracy: 0.9670 - val_loss: 4.6945 - val_accuracy: 0.9700\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 14s 214ms/step - loss: 4.6150 - accuracy: 0.9620 - val_loss: 4.0974 - val_accuracy: 0.9730\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 14s 214ms/step - loss: 3.5417 - accuracy: 0.9700 - val_loss: 3.9712 - val_accuracy: 0.9690\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 3.0648 - accuracy: 0.9730 - val_loss: 2.4376 - val_accuracy: 0.9790\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 2.6629 - accuracy: 0.9790 - val_loss: 3.8252 - val_accuracy: 0.9740\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 2.9059 - accuracy: 0.9765 - val_loss: 3.8302 - val_accuracy: 0.9750\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 2.7445 - accuracy: 0.9770 - val_loss: 4.5619 - val_accuracy: 0.9750\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 2.6691 - accuracy: 0.9750 - val_loss: 4.2773 - val_accuracy: 0.9790\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 1.7059 - accuracy: 0.9820 - val_loss: 2.9977 - val_accuracy: 0.9800\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 1.6131 - accuracy: 0.9845 - val_loss: 8.6564 - val_accuracy: 0.9650\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 14s 214ms/step - loss: 1.8065 - accuracy: 0.9835 - val_loss: 3.6176 - val_accuracy: 0.9780\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 1.6988 - accuracy: 0.9825 - val_loss: 3.8433 - val_accuracy: 0.9730\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 1.0626 - accuracy: 0.9870 - val_loss: 6.2762 - val_accuracy: 0.9700\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 14s 214ms/step - loss: 1.6564 - accuracy: 0.9855 - val_loss: 2.9758 - val_accuracy: 0.9820\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 1.0707 - accuracy: 0.9865 - val_loss: 13.7343 - val_accuracy: 0.9540\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 1.3795 - accuracy: 0.9865 - val_loss: 3.4744 - val_accuracy: 0.9780\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 1.7523 - accuracy: 0.9855 - val_loss: 4.2083 - val_accuracy: 0.9780\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 0.9433 - accuracy: 0.9870 - val_loss: 3.8893 - val_accuracy: 0.9780\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 1.3879 - accuracy: 0.9890 - val_loss: 5.1726 - val_accuracy: 0.9760\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.5104 - accuracy: 0.9930 - val_loss: 5.1293 - val_accuracy: 0.9780\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.8018 - accuracy: 0.9915 - val_loss: 4.8978 - val_accuracy: 0.9750\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.4836 - accuracy: 0.9935 - val_loss: 4.8662 - val_accuracy: 0.9740\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 1.0198 - accuracy: 0.9880 - val_loss: 4.9134 - val_accuracy: 0.9780\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.5759 - accuracy: 0.9895 - val_loss: 6.8280 - val_accuracy: 0.9720\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 1.3000 - accuracy: 0.9875 - val_loss: 4.5460 - val_accuracy: 0.9780\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 0.5533 - accuracy: 0.9935 - val_loss: 4.3334 - val_accuracy: 0.9810\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 0.5598 - accuracy: 0.9915 - val_loss: 5.1833 - val_accuracy: 0.9730\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.8683 - accuracy: 0.9905 - val_loss: 4.2987 - val_accuracy: 0.9760\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 0.7779 - accuracy: 0.9890 - val_loss: 3.9693 - val_accuracy: 0.9780\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 13s 211ms/step - loss: 1.0024 - accuracy: 0.9910 - val_loss: 4.0405 - val_accuracy: 0.9760\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 13s 212ms/step - loss: 0.5740 - accuracy: 0.9935 - val_loss: 4.7134 - val_accuracy: 0.9770\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.2638 - accuracy: 0.9960 - val_loss: 4.1608 - val_accuracy: 0.9750\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 14s 220ms/step - loss: 0.5543 - accuracy: 0.9920 - val_loss: 3.9396 - val_accuracy: 0.9770\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.9461 - accuracy: 0.9905 - val_loss: 4.6752 - val_accuracy: 0.9780\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.8478 - accuracy: 0.9905 - val_loss: 3.4329 - val_accuracy: 0.9770\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.4213 - accuracy: 0.9940 - val_loss: 3.9349 - val_accuracy: 0.9730\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.6631 - accuracy: 0.9930 - val_loss: 5.3741 - val_accuracy: 0.9740\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.8326 - accuracy: 0.9920 - val_loss: 3.1217 - val_accuracy: 0.9790\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 14s 219ms/step - loss: 0.6054 - accuracy: 0.9905 - val_loss: 4.3785 - val_accuracy: 0.9710\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.3923 - accuracy: 0.9945 - val_loss: 4.5647 - val_accuracy: 0.9770\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.2729 - accuracy: 0.9955 - val_loss: 4.8092 - val_accuracy: 0.9690\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.6759 - accuracy: 0.9895 - val_loss: 3.6997 - val_accuracy: 0.9730\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.6693 - accuracy: 0.9875 - val_loss: 3.5299 - val_accuracy: 0.9750\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.2871 - accuracy: 0.9955 - val_loss: 4.4750 - val_accuracy: 0.9740\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.6257 - accuracy: 0.9900 - val_loss: 5.4966 - val_accuracy: 0.9690\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.5700 - accuracy: 0.9910 - val_loss: 3.5408 - val_accuracy: 0.9740\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.6181 - accuracy: 0.9915 - val_loss: 4.0287 - val_accuracy: 0.9790\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.7378 - accuracy: 0.9910 - val_loss: 3.1615 - val_accuracy: 0.9760\n"
     ]
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(filepath=\"feature_extraction_augmented.keras\", save_best_only=True, monitor=\"val_loss\")]\n",
    "history = model.fit(train_dataset, epochs=50, validation_data=validation_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Evaluating the model on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 71ms/step - loss: 5.7719 - accuracy: 0.9700\n",
      "Test accuracy: 0.970\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model(\"feature_extraction_augmented.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter08_intro-to-dl-for-computer-vision.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
