{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing heatmaps of class activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow.keras.utils import get_file, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions\n",
    "from tensorflow.keras import Input, Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = Xception(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img_path, target_size):\n",
    "    \n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    array = img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = preprocess_input(array)\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = get_file(fname=\"elephant.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
    "image = image_to_array(img_path, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "[('n02504458', 'African_elephant', 0.86978954), ('n01871265', 'tusker', 0.077024326), ('n02504013', 'Indian_elephant', 0.02360241)]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(image)\n",
    "print(decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Setting up a model that returns the last convolutional output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "block1_conv1\n",
      "block1_conv1_bn\n",
      "block1_conv1_act\n",
      "block1_conv2\n",
      "block1_conv2_bn\n",
      "block1_conv2_act\n",
      "block2_sepconv1\n",
      "block2_sepconv1_bn\n",
      "block2_sepconv2_act\n",
      "block2_sepconv2\n",
      "block2_sepconv2_bn\n",
      "conv2d_8\n",
      "block2_pool\n",
      "batch_normalization_8\n",
      "add_24\n",
      "block3_sepconv1_act\n",
      "block3_sepconv1\n",
      "block3_sepconv1_bn\n",
      "block3_sepconv2_act\n",
      "block3_sepconv2\n",
      "block3_sepconv2_bn\n",
      "conv2d_9\n",
      "block3_pool\n",
      "batch_normalization_9\n",
      "add_25\n",
      "block4_sepconv1_act\n",
      "block4_sepconv1\n",
      "block4_sepconv1_bn\n",
      "block4_sepconv2_act\n",
      "block4_sepconv2\n",
      "block4_sepconv2_bn\n",
      "conv2d_10\n",
      "block4_pool\n",
      "batch_normalization_10\n",
      "add_26\n",
      "block5_sepconv1_act\n",
      "block5_sepconv1\n",
      "block5_sepconv1_bn\n",
      "block5_sepconv2_act\n",
      "block5_sepconv2\n",
      "block5_sepconv2_bn\n",
      "block5_sepconv3_act\n",
      "block5_sepconv3\n",
      "block5_sepconv3_bn\n",
      "add_27\n",
      "block6_sepconv1_act\n",
      "block6_sepconv1\n",
      "block6_sepconv1_bn\n",
      "block6_sepconv2_act\n",
      "block6_sepconv2\n",
      "block6_sepconv2_bn\n",
      "block6_sepconv3_act\n",
      "block6_sepconv3\n",
      "block6_sepconv3_bn\n",
      "add_28\n",
      "block7_sepconv1_act\n",
      "block7_sepconv1\n",
      "block7_sepconv1_bn\n",
      "block7_sepconv2_act\n",
      "block7_sepconv2\n",
      "block7_sepconv2_bn\n",
      "block7_sepconv3_act\n",
      "block7_sepconv3\n",
      "block7_sepconv3_bn\n",
      "add_29\n",
      "block8_sepconv1_act\n",
      "block8_sepconv1\n",
      "block8_sepconv1_bn\n",
      "block8_sepconv2_act\n",
      "block8_sepconv2\n",
      "block8_sepconv2_bn\n",
      "block8_sepconv3_act\n",
      "block8_sepconv3\n",
      "block8_sepconv3_bn\n",
      "add_30\n",
      "block9_sepconv1_act\n",
      "block9_sepconv1\n",
      "block9_sepconv1_bn\n",
      "block9_sepconv2_act\n",
      "block9_sepconv2\n",
      "block9_sepconv2_bn\n",
      "block9_sepconv3_act\n",
      "block9_sepconv3\n",
      "block9_sepconv3_bn\n",
      "add_31\n",
      "block10_sepconv1_act\n",
      "block10_sepconv1\n",
      "block10_sepconv1_bn\n",
      "block10_sepconv2_act\n",
      "block10_sepconv2\n",
      "block10_sepconv2_bn\n",
      "block10_sepconv3_act\n",
      "block10_sepconv3\n",
      "block10_sepconv3_bn\n",
      "add_32\n",
      "block11_sepconv1_act\n",
      "block11_sepconv1\n",
      "block11_sepconv1_bn\n",
      "block11_sepconv2_act\n",
      "block11_sepconv2\n",
      "block11_sepconv2_bn\n",
      "block11_sepconv3_act\n",
      "block11_sepconv3\n",
      "block11_sepconv3_bn\n",
      "add_33\n",
      "block12_sepconv1_act\n",
      "block12_sepconv1\n",
      "block12_sepconv1_bn\n",
      "block12_sepconv2_act\n",
      "block12_sepconv2\n",
      "block12_sepconv2_bn\n",
      "block12_sepconv3_act\n",
      "block12_sepconv3\n",
      "block12_sepconv3_bn\n",
      "add_34\n",
      "block13_sepconv1_act\n",
      "block13_sepconv1\n",
      "block13_sepconv1_bn\n",
      "block13_sepconv2_act\n",
      "block13_sepconv2\n",
      "block13_sepconv2_bn\n",
      "conv2d_11\n",
      "block13_pool\n",
      "batch_normalization_11\n",
      "add_35\n",
      "block14_sepconv1\n",
      "block14_sepconv1_bn\n",
      "block14_sepconv1_act\n",
      "block14_sepconv2\n",
      "block14_sepconv2_bn\n",
      "block14_sepconv2_act\n",
      "avg_pool\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "last_conv_layer = model.get_layer(\"block14_sepconv2_act\")\n",
    "last_conv_layer_model = Model(model.inputs, last_conv_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Reapplying the classifier on top of the last convolutional output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_pool/Mean:0\n",
      "predictions/Softmax:0\n"
     ]
    }
   ],
   "source": [
    "classifier_input = Input(shape=last_conv_layer.output.shape[1:])\n",
    "x = classifier_input\n",
    "\n",
    "for layer in [\"avg_pool\", \"predictions\"]:\n",
    "    x = model.get_layer(layer)(x)\n",
    "    print(x.name)\n",
    "    \n",
    "classifier_model = Model(classifier_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retrieving the gradients of the top predicted class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "with GradientTape() as tape:\n",
    "    \n",
    "    last_conv_layer_output = last_conv_layer_model(image)\n",
    "    tape.watch(last_conv_layer_output)\n",
    "    preds = classifier_model(last_conv_layer_output)\n",
    "    top_pred_index = tf.argmax(preds[0])\n",
    "    top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Gradient pooling and channel-importance weighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
    "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "\n",
    "for i in range(pooled_grads.shape[-1]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "    \n",
    "heatmap = np.mean(last_conv_layer_output, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Heatmap post-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28c3bb65fa0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Superimposing the heatmap on the original picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "img = load_img(img_path)\n",
    "img = img_to_array(img)\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "jet = cm.get_cmap(\"jet\")\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "jet_heatmap = array_to_img(jet_heatmap)\n",
    "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "jet_heatmap = img_to_array(jet_heatmap)\n",
    "\n",
    "superimposed_img = jet_heatmap * 0.4 + img\n",
    "superimposed_img = array_to_img(superimposed_img)\n",
    "\n",
    "save_path = \"elephant_cam.jpg\"\n",
    "superimposed_img.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter09_part03_interpreting-what-convnets-learn.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
